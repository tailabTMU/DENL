
# DENL: Diverse Ensemble and Noisy Logits

**Paper:** 
Mina Yazdani, Hamed Karimi, and Reza Samavi. "*DENL: Diverse Ensemble and Noisy Logits for Improved Robustness of Neural Networks*." Accepted in The 15th Asian Conference on Machine Learning (ACML 2023).


## Getting Started


### Prerequisites
The following python modules are required to run the code:
- Numpy (general array manipulations and utilities)
- Tensorflow 2.8.0
- Keras (neural network models)
- Matplotlib (graphing utilities)

### Running the experiments
Run the script ***main_of_mains_cross_validation.py*** for both training time and inference time.

### Files Descriptions
- ***main_of_mains_cross_validation.py***: This code file is the main code to run our proposed method and produce the results. To run the code, you need to 
set/use the flags to set up the details of the experiments. This code uses the following Python files:

- ***train_models_ensemble.py***: This code file contains the functions required for phase 1 training. 

- ***ensemble-training_utils.py***: This code file contains the functions required for phase 2 training.
 
- ***l2_attack.py***: This code file is to attack a network optimizing for $l_2$ distance.

- ***setup_cifar.py***, ***setup_mnist.py***: These code files are to prepare the datasets and CNN models for the experiments.

- ***test_ensemble_func.py***: This code file generates adversarial examples with two attack scenarios (single attack and superimposition attack).

- ***process_results.py***: This code file processes the results of the experiments.

## Cite
If you find the content useful for your research and applications, please cite us using this BibTeX:

```bibtex
@InProceedings{pmlr-v222-yazdani24a,
  title = 	 {{DENL}: {D}iverse Ensemble and Noisy Logits for Improved Robustness of Neural Networks},
  author =       {Yazdani, Mina and Karimi, Hamed and Samavi, Reza},
  booktitle = 	 {Proceedings of the 15th Asian Conference on Machine Learning},
  pages = 	 {1574--1589},
  year = 	 {2024},
  editor = 	 {Yanıkoğlu, Berrin and Buntine, Wray},
  volume = 	 {222},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {11--14 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v222/yazdani24a/yazdani24a.pdf},
  url = 	 {https://proceedings.mlr.press/v222/yazdani24a.html},
  abstract = 	 {Neural Networks (NN) are increasingly used for image classification in medical, transportation, and security devices. However, recent studies have revealed neural networks’ vulnerability against adversarial examples generated by adding small perturbations to images. These malicious samples are imperceptible by human eyes, but can give rise to misclassification by NN models. Defensive distillation is a defence mechanism in which the NN’s output probabilities are scaled to a user-defined range and used as labels to train a new model less sensitive to input perturbations. Despite initial success, defensive distillation was defeated by state-of-the-art attacks. A proposed countermeasure was to add noise in the inference time to hamper the adversarial attack which also decreased the model accuracy. In this paper, we address this limitation by proposing a two-phase training methodology to defend against adversarial attacks. In the first phase, we train architecturally diversified models individually using the cross-entropy loss function. In the second phase, we train the ensemble using a diversity-promoting loss function. Our experimental results show that our training methodology and noise addition in the inference time improved our ensemble’s resistance against adversarial attacks, while maintaining reasonable accuracy, compared to the state-of-the-art methods.}
}
```

